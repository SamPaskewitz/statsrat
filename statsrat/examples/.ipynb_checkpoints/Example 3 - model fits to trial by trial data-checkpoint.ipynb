{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsrat as sr\n",
    "from statsrat import rw\n",
    "from statsrat import expr\n",
    "\n",
    "# IMPORTANT NOTE: the \"data\" in this example are synthetic, i.e. generated by simulation rather than from\n",
    "# actual human participants.  This is to avoid any worries privacy issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT THE (SYNTHETIC) DATA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE MODELS\n",
    "\n",
    "# The derived attention model from Le Pelley, Mitchell, Beesley, George and Wills (2016).\n",
    "drva = rw.model(name = 'drva',\n",
    "                pred = rw.pred.identity,\n",
    "                fbase = rw.fbase.elem,\n",
    "                fweight = rw.fweight.none,\n",
    "                lrate = rw.lrate.from_aux_feature,\n",
    "                drate = rw.drate.zero,\n",
    "                aux = rw.aux.drva)\n",
    "\n",
    "# CompAct (with only elemental features); Model 4 from Paskewitz and Jones (2020).\n",
    "CompAct = rw.model(name = 'CompAct',\n",
    "                   pred = rw.pred.identity,\n",
    "                   fbase = rw.fbase.elem,\n",
    "                   fweight = rw.fweight.from_aux_norm,\n",
    "                   lrate = rw.lrate.from_aux_norm,\n",
    "                   drate = rw.drate.zero,\n",
    "                   aux = rw.aux.gradcomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE EXPERIMENT\n",
    "\n",
    "# Le Pelley and McLaren 2003 (learned predictiveness)\n",
    "# Test responses were really rating scales for both response options instead of choices.\n",
    "\n",
    "# ADD COMMENTS TO GIVE MORE DETAIL\n",
    "design = expr.schedule(resp_type = 'choice',\n",
    "                  stages = {\n",
    "                      'training': expr.stage(\n",
    "                            x_pn = [['a', 'v'], ['b', 'v'], ['a', 'w'], ['b', 'w'], ['c', 'x'], ['d', 'x'], ['c', 'y'], ['d', 'y']],\n",
    "                            y = 4*[['cat1'], ['cat2']],\n",
    "                            y_psb = ['cat1', 'cat2'],\n",
    "                            n_rep = 14),\n",
    "                      'transfer': expr.stage(\n",
    "                            x_pn = [['a', 'x'], ['b', 'y'], ['c', 'v'], ['d', 'w'], ['e', 'f'], ['g', 'h'], ['i', 'j'], ['k', 'l']],\n",
    "                            y = 4*[['cat3'], ['cat4']],\n",
    "                            y_psb = ['cat3', 'cat4'],\n",
    "                            n_rep = 4),\n",
    "                      'test': expr.stage(\n",
    "                            x_pn = [['a', 'c'], ['b', 'd'], ['v', 'x'], ['w', 'y'], ['e', 'h'], ['f', 'g'], ['i', 'j'], ['k', 'l']],\n",
    "                            y_psb = ['cat3', 'cat4'],\n",
    "                            lrn = False,\n",
    "                            n_rep = 1)\n",
    "                  })\n",
    "\n",
    "rel_irl = expr.oat(schedule_pos = ['design'],\n",
    "                  behav_score_pos = expr.behav_score(stage = 'test',\n",
    "                                                    trial_pos = ['a.c -> nothing', 'b.d -> nothing'],\n",
    "                                                    trial_neg = ['v.x -> nothing', 'w.y -> nothing'],\n",
    "                                                    resp_pos = ['cat3', 'cat4'],\n",
    "                                                    resp_neg = ['cat3', 'cat4']))\n",
    "\n",
    "lrn_pred = expr.experiment(schedules = {'design': design},\n",
    "                           oats = {'rel_irl': rel_irl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package statsrat:\n",
      "\n",
      "NAME\n",
      "    statsrat\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    bayes_regr (package)\n",
      "    examples (package)\n",
      "    exemplar (package)\n",
      "    expr (package)\n",
      "    latent_cause (package)\n",
      "    resp_fun\n",
      "    rw (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    fit_algorithm_plots(model, ds, x0=None, tau=None, n_time_intervals=6, time_interval_size=10, algorithm=8, algorithm_list=None)\n",
      "        Used to figure compare global optimization algorithms and/or test how long to\n",
      "        run global optimization (in fit_indv) by generating plots.\n",
      "        This should be run on a subset of the data prior to the main model fit.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model: object\n",
      "            Learning model.\n",
      "            \n",
      "        ds: dataset (xarray)\n",
      "            Dataset of time step level experimental data (cues, outcomes etc.)\n",
      "            for each participant.\n",
      "        \n",
      "        x0: data frame/array-like of floats or None, optional\n",
      "            Start points for each individual in the dataset.\n",
      "            If None, then parameter search starts at the midpoint\n",
      "            of each parameter's allowed interval.  Defaults to None\n",
      "        \n",
      "        tau: array-like of floats or None, optional\n",
      "            Natural parameters of the log-normal prior.\n",
      "            Defaults to None (to not use log-normal prior).\n",
      "            \n",
      "        n_time_intervals: int, optional\n",
      "            Number of time intervals to use for testing global optimization\n",
      "            (the global_time parameter of fit_indv).  Defaults to 6.\n",
      "            \n",
      "        time_interval_size: int, optional\n",
      "            Size of time intervals to test (in seconds).  Defaults to 10.\n",
      "            \n",
      "        algorithm: object or None, optional\n",
      "            The algorithm used for global optimization.  Defaults to nlopt.GD_STOGO.\n",
      "            Is ignored and can be None if algorithm_list (to compare multiple algorithms)\n",
      "            is specified instead.\n",
      "            \n",
      "        algorithm_list: list or None, optional\n",
      "            Used in place of the algorithm argument to specify a list of algorithms to compare.\n",
      "            Can be None (the default) if algorithm is specified instad (to only test only algorithm).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict containing:\n",
      "        \n",
      "        df: dataframe\n",
      "            Parameter estimates and log-likelihood/log-posterior values per person per\n",
      "            global optimization run time.\n",
      "            \n",
      "        plot: plotnine plot object\n",
      "            Plot of log-likelihood/log-posterior by optimization time, which can be\n",
      "            used to graphically assess convergence.\n",
      "            \n",
      "        Notes\n",
      "        -----\n",
      "        No local optimization is run.\n",
      "    \n",
      "    fit_em(model, ds, max_em_iter=5, global_time=15, local_time=15, algorithm=8)\n",
      "        Fit the model to time step data using the expectation-maximization (EM) algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model: object\n",
      "            Learning model.\n",
      "            \n",
      "        ds: dataset (xarray)\n",
      "            Dataset of time step level experimental data (cues, outcomes etc.)\n",
      "            for each participant.\n",
      "        \n",
      "        max_em_iter: int, optional\n",
      "            Maximum number of EM algorithm iterations.\n",
      "            Defaults to 5.\n",
      "            \n",
      "        global_time: int, optional\n",
      "            Maximum time (in seconds) per individual for global optimization.\n",
      "            Defaults to 15.\n",
      "            \n",
      "        local_time: int, optional\n",
      "            Maximum time (in seconds) per individual for local optimization.\n",
      "            Defaults to 15.\n",
      "            \n",
      "        algorithm: object, optional\n",
      "                The algorithm used for global optimization.  Defaults to nlopt.GD_STOGO.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This assumes that all (psychological) model parameters (when shifted to (0, Inf)) have a log-normal distribution.\n",
      "        \n",
      "        Let theta be defined as any model parameter, and y be that the natural logarithm of that \n",
      "        parameter after being shifted to the interval (0, Inf):\n",
      "        y = log(sign(theta - min theta)*(theta - min theta))\n",
      "        \n",
      "        Then we assume y ~ N(mu, 1/rho) where rho is a precision parameters.\n",
      "        The corresponding natural parameters are tau0 = mu*rho and tau1 = -0.5*rho.\n",
      "        \n",
      "        We perform the EM algorithm to estimate y, treating tau0 and tau1 as our latent variables,\n",
      "        where y' is the current estimate and x is the behavioral data:\n",
      "        Q(y | y') = E[log p(y | x, tau0, tau1)]\n",
      "        = log p(x | y) + E[tau0]*y + E[tau1]*y^2 + constant term with respect to y\n",
      "        This is obtained by using Bayes' theorem along with the canonical exponential form of the\n",
      "        log-normal prior.  Thus the E step consists of computing E[tau0] and E[tau1], where the\n",
      "        expectation is taken according to the posterior distribution of tau0 and tau1 (i.e. of mu and rho)\n",
      "        given x and y'.  Recognizing that this posterior is normal-gamma allows us to make the neccesary calculations\n",
      "        (details not provided here).\n",
      "    \n",
      "    fit_indv(model, ds, x0=None, tau=None, global_time=15, local_time=15, algorithm=8)\n",
      "        Fit the model to time step data by individual maximum likelihood\n",
      "        estimation (ML) or maximum a posteriori (MAP) estimation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model: object\n",
      "            Learning model.\n",
      "            \n",
      "        ds: dataset (xarray)\n",
      "            Dataset of time step level experimental data (cues, outcomes etc.)\n",
      "            for each participant.\n",
      "        \n",
      "        x0: data frame/array-like of floats or None, optional\n",
      "            Start points for each individual in the dataset.\n",
      "            If None, then parameter search starts at the midpoint\n",
      "            of each parameter's allowed interval.  Defaults to None\n",
      "        \n",
      "        tau: array-like of floats or None, optional\n",
      "            Natural parameters of the log-normal prior.\n",
      "            Defaults to None (don't use log-normal prior).\n",
      "            \n",
      "        global_time: int, optional\n",
      "            Maximum time (in seconds) per individual for global optimization.\n",
      "            Defaults to 15.\n",
      "            \n",
      "        local_time: int, optional\n",
      "            Maximum time (in seconds) per individual for local optimization.\n",
      "            Defaults to 15.\n",
      "            \n",
      "        algorithm: object, optional\n",
      "            The algorithm used for global optimization.  Defaults to nlopt.GD_STOGO.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        df: dataframe\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If tau is None (default) then MLE is performed (i.e. you use a uniform prior).\n",
      "        \n",
      "        This currently assumes log-normal priors on all model parameters.  This may be an\n",
      "        improper prior for some cases (e.g. a learning rate parameter that must be between\n",
      "        0 and 1 might be better modeled using something like a beta prior).  I may add different\n",
      "        types of prior in the future.\n",
      "        \n",
      "        For now, this assumes discrete choice data (i.e. resp_type = 'choice').\n",
      "        \n",
      "        The model is fitted by first using a global non-linear optimization algorithm (specified by the\n",
      "        'algorithm parameter', with GN_ORIG_DIRECT as default), and then a local non-linear optimization\n",
      "        algorithm (LN_SBPLX) for refining the answer.  Both algorithms are from the nlopt package.\n",
      "    \n",
      "    learn_plot(ds, var, sel=None, rename_coords=None, color_var=None, facet_var=None, draw_points=False, drop_zeros=False, only_main=False, stage_labels=True, text_size=15.0, figure_size=(4.0, 4.0), dodge_width=1.0)\n",
      "        Plots learning simulation data from a single schedule (condition, group) as a function of time.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        ds : dataset (xarray)\n",
      "            Learning simulation data (output of a model's 'simulate' method).\n",
      "        var : string\n",
      "            Variable to plot.\n",
      "        sel : dict, optional\n",
      "            Used to select a subset of 'var'.  Defaults to 'None' (i.e. all\n",
      "            data in 'var' are plotted).\n",
      "        rename_coords : dict or None, optional\n",
      "            Either a dictionary for re-naming coordinates (keys are old names and\n",
      "            values are new names), or None (don't re-name).  This dict must include\n",
      "            ALL variable names.  Defaults to None.\n",
      "        color_var : string, optional\n",
      "            Variable to be represented by color.\n",
      "            Defaults to None (see notes).\n",
      "        facet_var : string, optional\n",
      "            Variable to control faceting.\n",
      "            Defaults to None (see notes).\n",
      "        draw_points : boolean, optional\n",
      "            Whether or not points should be drawn as well as lines.\n",
      "            Defaults to False.\n",
      "        drop_zeros : boolean, optional\n",
      "            Drop rows where 'var' is zero.  Defaults to False.\n",
      "        only_main : boolean, optional\n",
      "            Only keep rows where 't_name' is 'main', i.e. time steps with\n",
      "            punctate cues and/or non-zero outcomes.  Defaults to False.\n",
      "        stage_labels : boolean, optional\n",
      "            Whether the x-axis should be labeled with 'stage_name' (if True) or\n",
      "            't', i.e. time step (if False).  Defaults to True.\n",
      "        text_size : float, optional\n",
      "            Specifies text size.  Defaults to 15.0.\n",
      "        figure_size : tuple of floats, optional\n",
      "            Figure width and height in inches.  Defaults to (4.0, 4.0).\n",
      "        dodge_width : float, optional\n",
      "            Amount to separate overlapping lines so that they appear visually\n",
      "            distinct (using Plotnine's position_dodge).  Defaults to 1.0.\n",
      "            \n",
      "        Returns\n",
      "        -------\n",
      "        plot : object\n",
      "            A plotnine plot object.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The variable plotted should not have more than two dimensions besides time step ('t').\n",
      "        \n",
      "        By default, the first non-time dimension will be used for color and the\n",
      "        second one for faceting.\n",
      "        \n",
      "        The 'sel' argument is used to index 'ds' via the latter's 'loc' method.\n",
      "        It should be a dictionary of the form {'dim0' : ['a', 'b'], 'dim1' : ['c']},\n",
      "        where 'dim0', 'dim1' etc. are one or more dimensions of 'var'.\n",
      "    \n",
      "    log_lik(model, ds, par_val)\n",
      "        Compute log-likelihood of individual time step data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : object\n",
      "            A learning model object.\n",
      "        \n",
      "        ds : dataset\n",
      "            Experimental data, including cues, behavioral responses,\n",
      "            outcomes etc. from one individual and schedule.\n",
      "        \n",
      "        par_val : list\n",
      "            Learning model parameters (floats or ints).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ll : float\n",
      "            Log-likelihood of the data given parameter values.\n",
      "    \n",
      "    make_sim_data(model, experiment, schedule=None, a_true=1, b_true=1, n=10)\n",
      "        Generate simulated data given an experiment and schedule (with random parameter vectors).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : object\n",
      "            Learning model.\n",
      "            \n",
      "        experiment : object\n",
      "            The experiment to be used for the recovery test.\n",
      "            \n",
      "        schedule_name : str, optional\n",
      "            Name of the experimental schedule to be used for the test.\n",
      "            Defaults to the first schedule in the experiment definition.\n",
      "            \n",
      "        a_true : int or list, optional\n",
      "            Hyperarameter of the beta distribution used to generate true\n",
      "            parameters.  Can be either a scalar or a list equal in length\n",
      "            to the the number of parameters.  Defaults to 1.\n",
      "            \n",
      "        b_true : int or list, optional\n",
      "            Hyperarameter of the beta distribution used to generate true\n",
      "            parameters.  Can be either a scalar or a list equal in length\n",
      "            to the the number of parameters.  Defaults to 1.\n",
      "            \n",
      "        n : int, optional\n",
      "            Number of individuals to simulate.  Defaults to 10.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For now, this assumes discrete choice data (i.e. resp_type = 'choice').\n",
      "        \n",
      "        If a = b = 1 (default), parameters will be drawn from uniform distributions.\n",
      "    \n",
      "    multi_plot(ds_list, var, sel=None, rename_coords=None, rename_schedules=None, schedule_facet=False, draw_points=False, drop_zeros=False, only_main=False, stage_labels=True, text_size=15.0, figure_size=(4.0, 4.0), dodge_width=1.0)\n",
      "        Plots learning simulation data from multiple schedules (conditions, groups) as a function of time.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        ds_list : list of datasets (xarray)\n",
      "            Each element of the list consists of learning simulation data\n",
      "            (output of a model's 'simulate' method) from a different schedule.\n",
      "        var : string\n",
      "            Variable to plot.\n",
      "        sel : list of dicts or None, optional\n",
      "            If a list, then elements correspond to elements of ds_list.\n",
      "            Each list element is either None (to include everything in the \n",
      "            corresponding data set) or a dict used to select a subset of 'var'.\n",
      "            Defaults to None (i.e. all data in 'var' are plotted for all data\n",
      "            sets).\n",
      "        rename_coords : dict or None, optional\n",
      "            Either a dictionary for re-naming coordinates (keys are old names and\n",
      "            values are new names), or None (don't re-name).  Defaults to None.\n",
      "        rename_schedules : dict or None, optional\n",
      "            Either a dictionary for re-naming schedules (keys are old names and\n",
      "            values are new names), or None (don't re-name).  Defaults to None.\n",
      "        schedule_facet : boolean, optional\n",
      "            Whether or not schedules should be on different facets instead\n",
      "            of on a single graph distinguished by color.  Defaults to False.\n",
      "        draw_points : boolean, optional\n",
      "            Whether or not points should be drawn as well as lines.\n",
      "            Defaults to False.\n",
      "        drop_zeros : boolean, optional\n",
      "            Drop rows where 'var' is zero.  Defaults to False.\n",
      "        only_main : boolean, optional\n",
      "            Only keep rows where 't_name' is 'main', i.e. time steps with\n",
      "            punctate cues and/or non-zero outcomes.  Defaults to False.\n",
      "        stage_labels : boolean, optional\n",
      "            Whether the x-axis should be labeled with 'stage_name' (if True) or\n",
      "            't', i.e. time step (if False).  Defaults to True.\n",
      "        text_size : float, optional\n",
      "            Specifies text size.  Defaults to 15.0.\n",
      "        figure_size : tuple of floats, optional\n",
      "            Figure width and height in inches.  Defaults to (4.0, 4.0).\n",
      "        dodge_width : float, optional\n",
      "            Amount to separate overlapping lines so that they appear visually\n",
      "            distinct (using Plotnine's position_dodge).  Defaults to 1.0.\n",
      "            \n",
      "        Returns\n",
      "        -------\n",
      "        plot : object\n",
      "            A plotnine plot object.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        It is assumed that either the schedules all have the same stage names, or else\n",
      "        that the 'sel' argument is used to select time steps that all have the same stage names.\n",
      "        \n",
      "        The variable plotted should not have more than one dimension besides time step ('t').\n",
      "        \n",
      "        If there is an extra dimension besides 't', it will be used for facetting (if\n",
      "        schedule_facet = False) or for color (if schedule_facet = True).\n",
      "        \n",
      "        The 'sel' argument is used to index 'ds' via the latter's 'loc' method.\n",
      "        It should be a dictionary of the form {'dim0' : ['a', 'b'], 'dim1' : ['c']},\n",
      "        where 'dim0', 'dim1' etc. are one or more dimensions of 'var'.\n",
      "    \n",
      "    multi_sim(model, trials_list, par_val, random_resp=False, sim_type=None)\n",
      "        Simulate one or more trial sequences from the same schedule with known parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : object\n",
      "            Model to use.\n",
      "        \n",
      "        trials_list : list\n",
      "            List of time step level experimental data (cues, outcomes\n",
      "            etc.) for each participant.  These should be generated from\n",
      "            the same experimental schedule.\n",
      "        \n",
      "        par_val : list\n",
      "            Learning model parameters (floats or ints).\n",
      "            \n",
      "        random_resp : boolean\n",
      "            Should responses be random?\n",
      "        \n",
      "        sim_type: str or None, optional\n",
      "            Type of simulation to perform (passed to the model's .simulate() method).\n",
      "            Should be a string indicating the type of simulation if there is more than\n",
      "            one type (e.g. latent cause models), and otherwise should be None.\n",
      "            Defaults to None.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ds : dataset\n",
      "    \n",
      "    oat_grid(model, experiment, free_par, fixed_values, n_points=10, oat=None, n=20)\n",
      "        Compute ordinal adequacy test (OAT) scores while varying one model parameter\n",
      "        (at evenly spaced intervals across its entire domain) and keeping the other parameters fixed.\n",
      "        Useful for examining model behavior via plots.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model: learning model object\n",
      "        \n",
      "        experiment: experiment\n",
      "        \n",
      "        free_par: str\n",
      "            Name of parameter to vary.\n",
      "            \n",
      "        fixed_values: dict\n",
      "            Dict of values to be given to fixed parameters (keys are\n",
      "            parameter names).\n",
      "            \n",
      "        n_points: int, optional\n",
      "            How many values of the free parameter should be\n",
      "            used.  Defaults to 10.\n",
      "        \n",
      "        oat: str, optional\n",
      "        \n",
      "        n: int, optional\n",
      "            Number of individuals to simulate.  Defaults to 20.\n",
      "            \n",
      "        Returns\n",
      "        -------\n",
      "        df: data frame\n",
      "            Parameter combinations with their mean OAT scores.\n",
      "    \n",
      "    one_step_pred(model, ds, n_pred=10, method='indv')\n",
      "        One step ahead prediction test (similar to cross-validation).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        ds : dataset (xarray)\n",
      "            Dataset of time step level experimental data (cues, outcomes etc.)\n",
      "            for each participant.\n",
      "        n_pred : int\n",
      "            The number of trials to be predicted (at the end of each data\n",
      "            set).\n",
      "        method : string\n",
      "            The method used to fit the model, either \"indv\" or \"em\".  Defaults\n",
      "            to 'indv'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This tests how well each of the last few choices is predicted by the model when fit to preceding trials.\n",
      "        \n",
      "        For now, this assumes discrete choice data (i.e. resp_type = 'choice')\n",
      "        \n",
      "        It is based on the 'prediction method' of Yechiam and Busemeyer (2005).\n",
      "        \n",
      "        We assume that each trial/response sequence has the same length.\n",
      "    \n",
      "    perform_oat(model, experiment, minimize=True, oat=None, n=5, max_time=60, verbose=False, algorithm=6, sim_type=None)\n",
      "        Perform an ordinal adequacy test (OAT).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model: learning model object\n",
      "        \n",
      "        experiment: experiment\n",
      "        \n",
      "        minimize: boolean, optional\n",
      "            Should the OAT score by minimized as well as maximized?\n",
      "            Defaults to True.\n",
      "        \n",
      "        oat: str or None, optional\n",
      "            Name of the OAT to use.  Defaults to None, in which\n",
      "            case the alphabetically first OAT in the experiment.\n",
      "        \n",
      "        n: int, optional\n",
      "            Number of individuals to simulate.  Defaults to 5.\n",
      "                \n",
      "        max_time: int, optional\n",
      "            Maximum time for each optimization (in seconds), i.e.\n",
      "            about half the maximum total time running the whole OAT should take.\n",
      "            Defaults to 60.\n",
      "            \n",
      "        verbose: boolean, optional\n",
      "            Should the parameter values be printed as the search is going on?\n",
      "            Defaults to False.\n",
      "            \n",
      "        algorithm: object, optional\n",
      "            NLopt algorithm to use for optimization.\n",
      "            Defaults to nlopt.GN_ORIG_DIRECT.\n",
      "            \n",
      "        sim_type: str or None, optional\n",
      "            Type of simulation to perform (passed to the model's .simulate() method).\n",
      "            Should be a string indicating the type of simulation if there is more than\n",
      "            one type (e.g. latent cause models), and otherwise should be None.\n",
      "            Defaults to None.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        output: dataframe (Pandas)\n",
      "            Model parameters that produce maximum and minimum mean OAT score,\n",
      "            along with those maximum and minimum mean OAT scores and (if n > 1)\n",
      "            their associated 95% confidence intervals.\n",
      "            \n",
      "        mean_resp_max: dataframe\n",
      "            Relevant responses at OAT maximum (and minimum if applicable), averaged\n",
      "            across individuals and trials.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The experiment's OAT object defines a behavioral score function\n",
      "        designed such that positive values reflect response patterns\n",
      "        consistent with empirical data and negative values reflect the\n",
      "        opposite.  This method maximizes and minimizes the score produced\n",
      "        by the learning model.  If the maximum score is positive, the model\n",
      "        CAN behave reproduce empirical results.  If the minimum score is\n",
      "        also positive, the model ALWAYS reproduces those results.\n",
      "    \n",
      "    recovery_test(model, experiment, schedule=None, a_true=1, b_true=1, n=10, method='indv')\n",
      "        Perform a parameter recovery test.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : object\n",
      "            Learning model.\n",
      "            \n",
      "        experiment : object\n",
      "            The experiment to be used for the recovery test.\n",
      "            \n",
      "        schedule : str, optional\n",
      "            Name of the experimental schedule to be used for the test.\n",
      "            Defaults to the first schedule in the experiment definition.\n",
      "            \n",
      "        a_true : int or list, optional\n",
      "            Hyperarameter of the beta distribution used to generate true\n",
      "            parameters.  Can be either a scalar or a list equal in length\n",
      "            to the the number of parameters.  Defaults to 1.\n",
      "            \n",
      "        b_true : int or list, optional\n",
      "            Hyperarameter of the beta distribution used to generate true\n",
      "            parameters.  Can be either a scalar or a list equal in length\n",
      "            to the the number of parameters.  Defaults to 1.\n",
      "            \n",
      "        n : int, optional\n",
      "            Number of individuals to simulate.  Defaults to 10.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        A parameter recovery test consists of three steps:\n",
      "        1) generate random parameter vectors (simulated individuals)\n",
      "        2) simulate data for each parameter vector\n",
      "        3) fit the model to the simulated data to estimate individual parameters\n",
      "        4) compare the estimated parameters (from step 3) to the true ones (from step 1)\n",
      "        This procedure allows one to test how well a given learning model's parameters\n",
      "        can be identified from data.  Some models and experimental schedules will have\n",
      "        better estimation properties than others.\n",
      "        \n",
      "        For now, this assumes discrete choice data (i.e. resp_type = 'choice').\n",
      "    \n",
      "    split_pred(model, trials_list, eresp_list, t_fit, method='indv')\n",
      "        Split prediction test (similar to cross-validation).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        trials_list : list\n",
      "            List of time step level experimental data (cues, outcomes\n",
      "            etc.) for each participant.\n",
      "        eresp_list : list\n",
      "            List of empirical response arrays for each participant.\n",
      "        t_fit : int\n",
      "            The first 't_fit' trials are used to predict the remaining\n",
      "            ones.\n",
      "        method : string\n",
      "            The method used to fit the model, either \"indv\" or \"em\".\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For now, this assumes discrete choice data (i.e. resp_type = 'choice').\n",
      "        \n",
      "        This is similar to the 'one_step_pred' method described above, but simply predict the last part of the data from the first.\n",
      "        \n",
      "        It is thus much faster to run and (at least for now) more practical.\n",
      "\n",
      "FILE\n",
      "    /Users/sam/Dropbox/Research/Modeling/PhD thesis/statsrat/statsrat/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_sim_data() got an unexpected keyword argument 'random_resp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-15fbde26c6ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msim_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_sim_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrn_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: make_sim_data() got an unexpected keyword argument 'random_resp'"
     ]
    }
   ],
   "source": [
    "sim_data = sr.make_sim_data(model = drva, experiment = lrn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method simulate in module statsrat.rw:\n",
      "\n",
      "simulate(trials, par_val=None, random_resp=False, ident='sim') method of statsrat.rw.model instance\n",
      "    Simulate a trial sequence once with known model parameters.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    trials: dataset (xarray)\n",
      "        Time step level experimental data (cues, outcomes etc.).\n",
      "    \n",
      "    par_val: list, optional\n",
      "        Learning model parameters (floats or ints).\n",
      "    \n",
      "    random_resp: str, optional\n",
      "        Whether or not simulated responses should be random.  Defaults\n",
      "        to false, in which case behavior (b) is identical to expected\n",
      "        behavior (b_hat); this saves some computation time.  If true\n",
      "        and resp_type is 'choice', then discrete responses are selected\n",
      "        using b_hat as choice probabilities.  If true and resp_type is\n",
      "        'exct' or 'supr' then a small amount of normally distributed\n",
      "        noise (sd = 0.01) is added to b_hat.\n",
      "    \n",
      "    ident: str, optional\n",
      "        Individual participant identifier.  Defaults to 'sim'.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    ds: dataset\n",
      "        Simulation data.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The response type is determined by the 'resp_type' attribute of the 'trials' object.\n",
      "    \n",
      "    The response type 'choice' is used for discrete response options.  This\n",
      "    produces response probabilities using a softmax function:\n",
      "    .. math::       ext{resp}_i = \f",
      "rac{ e^{\\phi \\hat{u}_i} }{ \\sum_j e^{\\phi \\hat{u}_j} }\n",
      "    \n",
      "    The response type 'exct' is used for excitatory Pavlovian\n",
      "    conditioning:\n",
      "    .. math::       ext{resp} = \f",
      "rac{ e^{\\phi \\hat{u}_i} }{ e^{\\phi \\hat{u}_i} + 1 }\n",
      "    \n",
      "    The response type 'supr' (suppression) is used for inhibitory\n",
      "    Pavlovian conditioning:\n",
      "    .. math::       ext{resp} = \f",
      "rac{ e^{-\\phi \\hat{u}_i} }{ e^{-\\phi \\hat{u}_i} + 1 }\n",
      "    \n",
      "    Here :math:`\\phi` represents the 'resp_scale' parameter.\n",
      "\n",
      "{'par': lvl0  true_par                     \n",
      "lvl1   atn_min     lrate resp_scale\n",
      "0     0.718629  0.044085   6.162878\n",
      "1     0.754779  0.267270   6.611132\n",
      "2     0.102026  0.443571   9.221315\n",
      "3     0.282307  0.592495   5.244979\n",
      "4     0.361205  0.969042   6.491595\n",
      "5     0.263266  0.477107   2.943805\n",
      "6     0.020323  0.466182   1.612881\n",
      "7     0.909595  0.015795   4.874959\n",
      "8     0.776091  0.965978   6.544642\n",
      "9     0.753460  0.371944   8.062128, 'ds': <xarray.Dataset>\n",
      "Dimensions:     (f_name: 16, ident: 10, t: 152, x_name: 16, y_name: 4)\n",
      "Coordinates:\n",
      "  * t           (t) int64 0 1 2 3 4 5 6 7 8 ... 144 145 146 147 148 149 150 151\n",
      "    t_name      (t) <U4 'main' 'main' 'main' 'main' ... 'main' 'main' 'main'\n",
      "    ex          (ident, t) <U3 'c.x' 'b.v' 'c.y' 'a.v' ... 'i.j' 'a.c' 'v.x'\n",
      "    trial       (t) int64 0 1 2 3 4 5 6 7 8 ... 144 145 146 147 148 149 150 151\n",
      "    trial_name  (ident, t) <U14 'c.x -> cat1' 'b.v -> cat2' ... 'v.x -> nothing'\n",
      "    stage       (t) int64 0 0 0 0 0 0 0 0 0 0 0 ... 8 8 16 16 16 16 16 16 16 16\n",
      "    stage_name  (t) <U8 'training' 'training' 'training' ... 'test' 'test'\n",
      "  * x_name      (x_name) <U1 'a' 'b' 'c' 'd' 'e' 'f' ... 'k' 'l' 'v' 'w' 'x' 'y'\n",
      "  * y_name      (y_name) <U4 'cat1' 'cat2' 'cat3' 'cat4'\n",
      "    time        (t) int64 0 1 2 3 4 5 6 7 8 ... 144 145 146 147 148 149 150 151\n",
      "  * f_name      (f_name) <U1 'a' 'b' 'c' 'd' 'e' 'f' ... 'k' 'l' 'v' 'w' 'x' 'y'\n",
      "  * ident       (ident) <U4 'sim0' 'sim1' 'sim2' 'sim3' ... 'sim7' 'sim8' 'sim9'\n",
      "Data variables: (12/15)\n",
      "    x           (ident, t, x_name) float64 0.0 0.0 1.0 0.0 ... 1.0 0.0 1.0 0.0\n",
      "    y           (ident, t, y_name) float64 1.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
      "    y_psb       (ident, t, y_name) float64 1.0 1.0 0.0 0.0 ... 0.0 0.0 1.0 1.0\n",
      "    y_lrn       (ident, t, y_name) float64 1.0 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
      "    fbase       (ident, t, f_name) float64 0.0 0.0 1.0 0.0 ... 1.0 0.0 1.0 0.0\n",
      "    fweight     (ident, t, f_name) float64 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0\n",
      "    ...          ...\n",
      "    b           (ident, t, y_name) float64 0.5 0.5 0.0 ... 0.0 0.9991 0.0009473\n",
      "    w           (ident, t, f_name, y_name) float64 0.0 0.0 0.0 ... 0.0 0.4318\n",
      "    delta       (ident, t, y_name) float64 1.0 0.0 0.0 0.0 ... 0.0 -0.8634 0.0\n",
      "    lrate       (ident, t, f_name, y_name) float64 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    drate       (ident, t, f_name, y_name) float64 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    atn         (ident, t, f_name) float64 0.7186 0.7186 ... 0.9311 0.9312}\n"
     ]
    }
   ],
   "source": [
    "help(CompAct.simulate)\n",
    "print(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  x    y  y_psb  y_lrn t_name   ex  trial  \\\n",
      "f_name ident t   x_name y_name                                              \n",
      "a      sim0  0   a      cat1    0.0  1.0    1.0    1.0   main  c.x      0   \n",
      "                        cat2    0.0  0.0    1.0    1.0   main  c.x      0   \n",
      "                        cat3    0.0  0.0    0.0    0.0   main  c.x      0   \n",
      "                        cat4    0.0  0.0    0.0    0.0   main  c.x      0   \n",
      "                 b      cat1    0.0  1.0    1.0    1.0   main  c.x      0   \n",
      "...                             ...  ...    ...    ...    ...  ...    ...   \n",
      "y      sim9  151 x      cat4    1.0  0.0    1.0    0.0   main  v.x    151   \n",
      "                 y      cat1    0.0  0.0    0.0    0.0   main  v.x    151   \n",
      "                        cat2    0.0  0.0    0.0    0.0   main  v.x    151   \n",
      "                        cat3    0.0  0.0    1.0    0.0   main  v.x    151   \n",
      "                        cat4    0.0  0.0    1.0    0.0   main  v.x    151   \n",
      "\n",
      "                                    trial_name  stage stage_name  ...  \\\n",
      "f_name ident t   x_name y_name                                    ...   \n",
      "a      sim0  0   a      cat1       c.x -> cat1      0   training  ...   \n",
      "                        cat2       c.x -> cat1      0   training  ...   \n",
      "                        cat3       c.x -> cat1      0   training  ...   \n",
      "                        cat4       c.x -> cat1      0   training  ...   \n",
      "                 b      cat1       c.x -> cat1      0   training  ...   \n",
      "...                                        ...    ...        ...  ...   \n",
      "y      sim9  151 x      cat4    v.x -> nothing     16       test  ...   \n",
      "                 y      cat1    v.x -> nothing     16       test  ...   \n",
      "                        cat2    v.x -> nothing     16       test  ...   \n",
      "                        cat3    v.x -> nothing     16       test  ...   \n",
      "                        cat4    v.x -> nothing     16       test  ...   \n",
      "\n",
      "                                fweight  f_x     y_hat     b_hat         b  \\\n",
      "f_name ident t   x_name y_name                                               \n",
      "a      sim0  0   a      cat1        1.0  0.0  0.000000  0.500000  0.500000   \n",
      "                        cat2        1.0  0.0  0.000000  0.500000  0.500000   \n",
      "                        cat3        1.0  0.0  0.000000  0.000000  0.000000   \n",
      "                        cat4        1.0  0.0  0.000000  0.000000  0.000000   \n",
      "                 b      cat1        1.0  0.0  0.000000  0.500000  0.500000   \n",
      "...                                 ...  ...       ...       ...       ...   \n",
      "y      sim9  151 x      cat4        1.0  0.0  0.000000  0.000947  0.000947   \n",
      "                 y      cat1        1.0  0.0  0.000000  0.000000  0.000000   \n",
      "                        cat2        1.0  0.0  0.000000  0.000000  0.000000   \n",
      "                        cat3        1.0  0.0  0.863416  0.999053  0.999053   \n",
      "                        cat4        1.0  0.0  0.000000  0.000947  0.000947   \n",
      "\n",
      "                                       w     delta  lrate  drate       atn  \n",
      "f_name ident t   x_name y_name                                              \n",
      "a      sim0  0   a      cat1    0.000000  1.000000    0.0    0.0  0.718629  \n",
      "                        cat2    0.000000  0.000000    0.0    0.0  0.718629  \n",
      "                        cat3    0.000000  0.000000    0.0    0.0  0.718629  \n",
      "                        cat4    0.000000  0.000000    0.0    0.0  0.718629  \n",
      "                 b      cat1    0.000000  1.000000    0.0    0.0  0.718629  \n",
      "...                                  ...       ...    ...    ...       ...  \n",
      "y      sim9  151 x      cat4    0.431785  0.000000    0.0    0.0  0.931175  \n",
      "                 y      cat1    0.249873  0.000000    0.0    0.0  0.931175  \n",
      "                        cat2    0.249516  0.000000    0.0    0.0  0.931175  \n",
      "                        cat3    0.000000 -0.863416    0.0    0.0  0.931175  \n",
      "                        cat4    0.431785  0.000000    0.0    0.0  0.931175  \n",
      "\n",
      "[1556480 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "bar = sim_data['ds'].to_dataframe()\n",
    "print(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method simulate in module statsrat.rw:\n",
      "\n",
      "simulate(trials, par_val=None, random_resp=False, ident='sim') method of statsrat.rw.model instance\n",
      "    Simulate a trial sequence once with known model parameters.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    trials: dataset (xarray)\n",
      "        Time step level experimental data (cues, outcomes etc.).\n",
      "    \n",
      "    par_val: list, optional\n",
      "        Learning model parameters (floats or ints).\n",
      "    \n",
      "    random_resp: str, optional\n",
      "        Whether or not simulated responses should be random.  Defaults\n",
      "        to false, in which case behavior (b) is identical to expected\n",
      "        behavior (b_hat); this saves some computation time.  If true\n",
      "        and resp_type is 'choice', then discrete responses are selected\n",
      "        using b_hat as choice probabilities.  If true and resp_type is\n",
      "        'exct' or 'supr' then a small amount of normally distributed\n",
      "        noise (sd = 0.01) is added to b_hat.\n",
      "    \n",
      "    ident: str, optional\n",
      "        Individual participant identifier.  Defaults to 'sim'.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    ds: dataset\n",
      "        Simulation data.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The response type is determined by the 'resp_type' attribute of the 'trials' object.\n",
      "    \n",
      "    The response type 'choice' is used for discrete response options.  This\n",
      "    produces response probabilities using a softmax function:\n",
      "    .. math::       ext{resp}_i = \f",
      "rac{ e^{\\phi \\hat{u}_i} }{ \\sum_j e^{\\phi \\hat{u}_j} }\n",
      "    \n",
      "    The response type 'exct' is used for excitatory Pavlovian\n",
      "    conditioning:\n",
      "    .. math::       ext{resp} = \f",
      "rac{ e^{\\phi \\hat{u}_i} }{ e^{\\phi \\hat{u}_i} + 1 }\n",
      "    \n",
      "    The response type 'supr' (suppression) is used for inhibitory\n",
      "    Pavlovian conditioning:\n",
      "    .. math::       ext{resp} = \f",
      "rac{ e^{-\\phi \\hat{u}_i} }{ e^{-\\phi \\hat{u}_i} + 1 }\n",
      "    \n",
      "    Here :math:`\\phi` represents the 'resp_scale' parameter.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CompAct.simulate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a subset of the data for compare optimization algorithms etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST MODEL (**MODEL NAME**)\n",
    "\n",
    "# Test different optimization algorithms (subset of data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how long the optimization algorithm needs to run (subset of data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data (full dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND MODEL (**MODEL NAME**)\n",
    "\n",
    "# Test different optimization algorithms (subset of data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how long the optimization algorithm needs to run (subset of data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data (full dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare AIC (Akaike Information Criterion) values\n",
    "# These are based on a log-likelihood but penalize the number of free parameters\n",
    "# Higher is better\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
