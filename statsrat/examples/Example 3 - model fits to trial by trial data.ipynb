{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsrat as sr\n",
    "from statsrat import rw\n",
    "from statsrat import expr\n",
    "\n",
    "# IMPORTANT NOTE: the \"data\" in this example are synthetic, i.e. generated by simulation rather than from\n",
    "# actual human participants.  This is to avoid any worries privacy issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT THE (SYNTHETIC) DATA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE MODELS\n",
    "\n",
    "# The derived attention model from Le Pelley, Mitchell, Beesley, George and Wills (2016).\n",
    "drva = rw.model(name = 'drva',\n",
    "                pred = rw.pred.identity,\n",
    "                fbase = rw.fbase.elem,\n",
    "                fweight = rw.fweight.none,\n",
    "                lrate = rw.lrate.from_aux_feature,\n",
    "                drate = rw.drate.zero,\n",
    "                aux = rw.aux.drva)\n",
    "\n",
    "# CompAct (with only elemental features); Model 4 from Paskewitz and Jones (2020).\n",
    "CompAct = rw.model(name = 'CompAct',\n",
    "                   pred = rw.pred.identity,\n",
    "                   fbase = rw.fbase.elem,\n",
    "                   fweight = rw.fweight.from_aux_norm,\n",
    "                   lrate = rw.lrate.from_aux_norm,\n",
    "                   drate = rw.drate.zero,\n",
    "                   aux = rw.aux.gradcomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE EXPERIMENT\n",
    "\n",
    "# Le Pelley and McLaren 2003 (learned predictiveness)\n",
    "# Test responses were really rating scales for both response options instead of choices.\n",
    "\n",
    "# ADD COMMENTS TO GIVE MORE DETAIL\n",
    "design = expr.schedule(resp_type = 'choice',\n",
    "                  stages = {\n",
    "                      'training': expr.stage(\n",
    "                            x_pn = [['a', 'v'], ['b', 'v'], ['a', 'w'], ['b', 'w'], ['c', 'x'], ['d', 'x'], ['c', 'y'], ['d', 'y']],\n",
    "                            y = 4*[['cat1'], ['cat2']],\n",
    "                            y_psb = ['cat1', 'cat2'],\n",
    "                            n_rep = 14),\n",
    "                      'transfer': expr.stage(\n",
    "                            x_pn = [['a', 'x'], ['b', 'y'], ['c', 'v'], ['d', 'w'], ['e', 'f'], ['g', 'h'], ['i', 'j'], ['k', 'l']],\n",
    "                            y = 4*[['cat3'], ['cat4']],\n",
    "                            y_psb = ['cat3', 'cat4'],\n",
    "                            n_rep = 4),\n",
    "                      'test': expr.stage(\n",
    "                            x_pn = [['a', 'c'], ['b', 'd'], ['v', 'x'], ['w', 'y'], ['e', 'h'], ['f', 'g'], ['i', 'j'], ['k', 'l']],\n",
    "                            y_psb = ['cat3', 'cat4'],\n",
    "                            lrn = False,\n",
    "                            n_rep = 1)\n",
    "                  })\n",
    "\n",
    "rel_irl = expr.oat(schedule_pos = ['design'],\n",
    "                  behav_score_pos = expr.behav_score(stage = 'test',\n",
    "                                                    trial_pos = ['a.c -> nothing', 'b.d -> nothing'],\n",
    "                                                    trial_neg = ['v.x -> nothing', 'w.y -> nothing'],\n",
    "                                                    resp_pos = ['cat3', 'cat4'],\n",
    "                                                    resp_neg = ['cat3', 'cat4']))\n",
    "\n",
    "lrn_pred = expr.experiment(schedules = {'design': design},\n",
    "                           oats = {'rel_irl': rel_irl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package statsrat:\n",
      "\n",
      "NAME\n",
      "    statsrat\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    bayes_regr (package)\n",
      "    examples (package)\n",
      "    exemplar (package)\n",
      "    expr (package)\n",
      "    latent_cause (package)\n",
      "    resp_fun\n",
      "    rw (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    fit_algorithm_plots(model, ds, x0=None, tau=None, n_time_intervals=6, time_interval_size=10, algorithm=8, algorithm_list=None)\n",
      "        Used to figure compare global optimization algorithms and/or test how long to\n",
      "        run global optimization (in fit_indv) by generating plots.\n",
      "        This should be run on a subset of the data prior to the main model fit.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model: object\n",
      "            Learning model.\n",
      "            \n",
      "        ds: dataset (xarray)\n",
      "            Dataset of time step level experimental data (cues, outcomes etc.)\n",
      "            for each participant.\n",
      "        \n",
      "        x0: data frame/array-like of floats or None, optional\n",
      "            Start points for each individual in the dataset.\n",
      "            If None, then parameter search starts at the midpoint\n",
      "            of each parameter's allowed interval.  Defaults to None\n",
      "        \n",
      "        tau: array-like of floats or None, optional\n",
      "            Natural parameters of the log-normal prior.\n",
      "            Defaults to None (to not use log-normal prior).\n",
      "            \n",
      "        n_time_intervals: int, optional\n",
      "            Number of time intervals to use for testing global optimization\n",
      "            (the global_time parameter of fit_indv).  Defaults to 6.\n",
      "            \n",
      "        time_interval_size: int, optional\n",
      "            Size of time intervals to test (in seconds).  Defaults to 10.\n",
      "            \n",
      "        algorithm: object or None, optional\n",
      "            The algorithm used for global optimization.  Defaults to nlopt.GD_STOGO.\n",
      "            Is ignored and can be None if algorithm_list (to compare multiple algorithms)\n",
      "            is specified instead.\n",
      "            \n",
      "        algorithm_list: list or None, optional\n",
      "            Used in place of the algorithm argument to specify a list of algorithms to compare.\n",
      "            Can be None (the default) if algorithm is specified instad (to only test only algorithm).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict containing:\n",
      "        \n",
      "        df: dataframe\n",
      "            Parameter estimates and log-likelihood/log-posterior values per person per\n",
      "            global optimization run time.\n",
      "            \n",
      "        plot: plotnine plot object\n",
      "            Plot of log-likelihood/log-posterior by optimization time, which can be\n",
      "            used to graphically assess convergence.\n",
      "            \n",
      "        Notes\n",
      "        -----\n",
      "        No local optimization is run.\n",
      "    \n",
      "    fit_em(model, ds, max_em_iter=5, global_time=15, local_time=15, algorithm=8)\n",
      "        Fit the model to time step data using the expectation-maximization (EM) algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model: object\n",
      "            Learning model.\n",
      "            \n",
      "        ds: dataset (xarray)\n",
      "            Dataset of time step level experimental data (cues, outcomes etc.)\n",
      "            for each participant.\n",
      "        \n",
      "        max_em_iter: int, optional\n",
      "            Maximum number of EM algorithm iterations.\n",
      "            Defaults to 5.\n",
      "            \n",
      "        global_time: int, optional\n",
      "            Maximum time (in seconds) per individual for global optimization.\n",
      "            Defaults to 15.\n",
      "            \n",
      "        local_time: int, optional\n",
      "            Maximum time (in seconds) per individual for local optimization.\n",
      "            Defaults to 15.\n",
      "            \n",
      "        algorithm: object, optional\n",
      "                The algorithm used for global optimization.  Defaults to nlopt.GD_STOGO.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This assumes that all (psychological) model parameters (when shifted to (0, Inf)) have a log-normal distribution.\n",
      "        \n",
      "        Let theta be defined as any model parameter, and y be that the natural logarithm of that \n",
      "        parameter after being shifted to the interval (0, Inf):\n",
      "        y = log(sign(theta - min theta)*(theta - min theta))\n",
      "        \n",
      "        Then we assume y ~ N(mu, 1/rho) where rho is a precision parameters.\n",
      "        The corresponding natural parameters are tau0 = mu*rho and tau1 = -0.5*rho.\n",
      "        \n",
      "        We perform the EM algorithm to estimate y, treating tau0 and tau1 as our latent variables,\n",
      "        where y' is the current estimate and x is the behavioral data:\n",
      "        Q(y | y') = E[log p(y | x, tau0, tau1)]\n",
      "        = log p(x | y) + E[tau0]*y + E[tau1]*y^2 + constant term with respect to y\n",
      "        This is obtained by using Bayes' theorem along with the canonical exponential form of the\n",
      "        log-normal prior.  Thus the E step consists of computing E[tau0] and E[tau1], where the\n",
      "        expectation is taken according to the posterior distribution of tau0 and tau1 (i.e. of mu and rho)\n",
      "        given x and y'.  Recognizing that this posterior is normal-gamma allows us to make the neccesary calculations\n",
      "        (details not provided here).\n",
      "    \n",
      "    fit_indv(model, ds, x0=None, tau=None, global_time=15, local_time=15, algorithm=8)\n",
      "        Fit the model to time step data by individual maximum likelihood\n",
      "        estimation (ML) or maximum a posteriori (MAP) estimation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model: object\n",
      "            Learning model.\n",
      "            \n",
      "        ds: dataset (xarray)\n",
      "            Dataset of time step level experimental data (cues, outcomes etc.)\n",
      "            for each participant.\n",
      "        \n",
      "        x0: data frame/array-like of floats or None, optional\n",
      "            Start points for each individual in the dataset.\n",
      "            If None, then parameter search starts at the midpoint\n",
      "            of each parameter's allowed interval.  Defaults to None\n",
      "        \n",
      "        tau: array-like of floats or None, optional\n",
      "            Natural parameters of the log-normal prior.\n",
      "            Defaults to None (don't use log-normal prior).\n",
      "            \n",
      "        global_time: int, optional\n",
      "            Maximum time (in seconds) per individual for global optimization.\n",
      "            Defaults to 15.\n",
      "            \n",
      "        local_time: int, optional\n",
      "            Maximum time (in seconds) per individual for local optimization.\n",
      "            Defaults to 15.\n",
      "            \n",
      "        algorithm: object, optional\n",
      "            The algorithm used for global optimization.  Defaults to nlopt.GD_STOGO.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        df: dataframe\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If tau is None (default) then MLE is performed (i.e. you use a uniform prior).\n",
      "        \n",
      "        This currently assumes log-normal priors on all model parameters.  This may be an\n",
      "        improper prior for some cases (e.g. a learning rate parameter that must be between\n",
      "        0 and 1 might be better modeled using something like a beta prior).  I may add different\n",
      "        types of prior in the future.\n",
      "        \n",
      "        For now, this assumes discrete choice data (i.e. resp_type = 'choice').\n",
      "        \n",
      "        The model is fitted by first using a global non-linear optimization algorithm (specified by the\n",
      "        'algorithm parameter', with GN_ORIG_DIRECT as default), and then a local non-linear optimization\n",
      "        algorithm (LN_SBPLX) for refining the answer.  Both algorithms are from the nlopt package.\n",
      "    \n",
      "    learn_plot(ds, var, sel=None, rename_coords=None, color_var=None, facet_var=None, draw_points=False, drop_zeros=False, only_main=False, stage_labels=True, text_size=15.0, figure_size=(4.0, 4.0), dodge_width=1.0)\n",
      "        Plots learning simulation data from a single schedule (condition, group) as a function of time.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        ds : dataset (xarray)\n",
      "            Learning simulation data (output of a model's 'simulate' method).\n",
      "        var : string\n",
      "            Variable to plot.\n",
      "        sel : dict, optional\n",
      "            Used to select a subset of 'var'.  Defaults to 'None' (i.e. all\n",
      "            data in 'var' are plotted).\n",
      "        rename_coords : dict or None, optional\n",
      "            Either a dictionary for re-naming coordinates (keys are old names and\n",
      "            values are new names), or None (don't re-name).  This dict must include\n",
      "            ALL variable names.  Defaults to None.\n",
      "        color_var : string, optional\n",
      "            Variable to be represented by color.\n",
      "            Defaults to None (see notes).\n",
      "        facet_var : string, optional\n",
      "            Variable to control faceting.\n",
      "            Defaults to None (see notes).\n",
      "        draw_points : boolean, optional\n",
      "            Whether or not points should be drawn as well as lines.\n",
      "            Defaults to False.\n",
      "        drop_zeros : boolean, optional\n",
      "            Drop rows where 'var' is zero.  Defaults to False.\n",
      "        only_main : boolean, optional\n",
      "            Only keep rows where 't_name' is 'main', i.e. time steps with\n",
      "            punctate cues and/or non-zero outcomes.  Defaults to False.\n",
      "        stage_labels : boolean, optional\n",
      "            Whether the x-axis should be labeled with 'stage_name' (if True) or\n",
      "            't', i.e. time step (if False).  Defaults to True.\n",
      "        text_size : float, optional\n",
      "            Specifies text size.  Defaults to 15.0.\n",
      "        figure_size : tuple of floats, optional\n",
      "            Figure width and height in inches.  Defaults to (4.0, 4.0).\n",
      "        dodge_width : float, optional\n",
      "            Amount to separate overlapping lines so that they appear visually\n",
      "            distinct (using Plotnine's position_dodge).  Defaults to 1.0.\n",
      "            \n",
      "        Returns\n",
      "        -------\n",
      "        plot : object\n",
      "            A plotnine plot object.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The variable plotted should not have more than two dimensions besides time step ('t').\n",
      "        \n",
      "        By default, the first non-time dimension will be used for color and the\n",
      "        second one for faceting.\n",
      "        \n",
      "        The 'sel' argument is used to index 'ds' via the latter's 'loc' method.\n",
      "        It should be a dictionary of the form {'dim0' : ['a', 'b'], 'dim1' : ['c']},\n",
      "        where 'dim0', 'dim1' etc. are one or more dimensions of 'var'.\n",
      "    \n",
      "    log_lik(model, ds, par_val)\n",
      "        Compute log-likelihood of individual time step data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : object\n",
      "            A learning model object.\n",
      "        \n",
      "        ds : dataset\n",
      "            Experimental data, including cues, behavioral responses,\n",
      "            outcomes etc. from one individual and schedule.\n",
      "        \n",
      "        par_val : list\n",
      "            Learning model parameters (floats or ints).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ll : float\n",
      "            Log-likelihood of the data given parameter values.\n",
      "    \n",
      "    make_sim_data(model, experiment, schedule=None, a_true=1, b_true=1, n=10)\n",
      "        Generate simulated data given an experiment and schedule (with random parameter vectors).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : object\n",
      "            Learning model.\n",
      "            \n",
      "        experiment : object\n",
      "            The experiment to be used for the recovery test.\n",
      "            \n",
      "        schedule_name : str, optional\n",
      "            Name of the experimental schedule to be used for the test.\n",
      "            Defaults to the first schedule in the experiment definition.\n",
      "            \n",
      "        a_true : int or list, optional\n",
      "            Hyperarameter of the beta distribution used to generate true\n",
      "            parameters.  Can be either a scalar or a list equal in length\n",
      "            to the the number of parameters.  Defaults to 1.\n",
      "            \n",
      "        b_true : int or list, optional\n",
      "            Hyperarameter of the beta distribution used to generate true\n",
      "            parameters.  Can be either a scalar or a list equal in length\n",
      "            to the the number of parameters.  Defaults to 1.\n",
      "            \n",
      "        n : int, optional\n",
      "            Number of individuals to simulate.  Defaults to 10.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For now, this assumes discrete choice data (i.e. resp_type = 'choice').\n",
      "        \n",
      "        If a = b = 1 (default), parameters will be drawn from uniform distributions.\n",
      "    \n",
      "    multi_plot(ds_list, var, sel=None, rename_coords=None, rename_schedules=None, schedule_facet=False, draw_points=False, drop_zeros=False, only_main=False, stage_labels=True, text_size=15.0, figure_size=(4.0, 4.0), dodge_width=1.0)\n",
      "        Plots learning simulation data from multiple schedules (conditions, groups) as a function of time.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        ds_list : list of datasets (xarray)\n",
      "            Each element of the list consists of learning simulation data\n",
      "            (output of a model's 'simulate' method) from a different schedule.\n",
      "        var : string\n",
      "            Variable to plot.\n",
      "        sel : list of dicts or None, optional\n",
      "            If a list, then elements correspond to elements of ds_list.\n",
      "            Each list element is either None (to include everything in the \n",
      "            corresponding data set) or a dict used to select a subset of 'var'.\n",
      "            Defaults to None (i.e. all data in 'var' are plotted for all data\n",
      "            sets).\n",
      "        rename_coords : dict or None, optional\n",
      "            Either a dictionary for re-naming coordinates (keys are old names and\n",
      "            values are new names), or None (don't re-name).  Defaults to None.\n",
      "        rename_schedules : dict or None, optional\n",
      "            Either a dictionary for re-naming schedules (keys are old names and\n",
      "            values are new names), or None (don't re-name).  Defaults to None.\n",
      "        schedule_facet : boolean, optional\n",
      "            Whether or not schedules should be on different facets instead\n",
      "            of on a single graph distinguished by color.  Defaults to False.\n",
      "        draw_points : boolean, optional\n",
      "            Whether or not points should be drawn as well as lines.\n",
      "            Defaults to False.\n",
      "        drop_zeros : boolean, optional\n",
      "            Drop rows where 'var' is zero.  Defaults to False.\n",
      "        only_main : boolean, optional\n",
      "            Only keep rows where 't_name' is 'main', i.e. time steps with\n",
      "            punctate cues and/or non-zero outcomes.  Defaults to False.\n",
      "        stage_labels : boolean, optional\n",
      "            Whether the x-axis should be labeled with 'stage_name' (if True) or\n",
      "            't', i.e. time step (if False).  Defaults to True.\n",
      "        text_size : float, optional\n",
      "            Specifies text size.  Defaults to 15.0.\n",
      "        figure_size : tuple of floats, optional\n",
      "            Figure width and height in inches.  Defaults to (4.0, 4.0).\n",
      "        dodge_width : float, optional\n",
      "            Amount to separate overlapping lines so that they appear visually\n",
      "            distinct (using Plotnine's position_dodge).  Defaults to 1.0.\n",
      "            \n",
      "        Returns\n",
      "        -------\n",
      "        plot : object\n",
      "            A plotnine plot object.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        It is assumed that either the schedules all have the same stage names, or else\n",
      "        that the 'sel' argument is used to select time steps that all have the same stage names.\n",
      "        \n",
      "        The variable plotted should not have more than one dimension besides time step ('t').\n",
      "        \n",
      "        If there is an extra dimension besides 't', it will be used for facetting (if\n",
      "        schedule_facet = False) or for color (if schedule_facet = True).\n",
      "        \n",
      "        The 'sel' argument is used to index 'ds' via the latter's 'loc' method.\n",
      "        It should be a dictionary of the form {'dim0' : ['a', 'b'], 'dim1' : ['c']},\n",
      "        where 'dim0', 'dim1' etc. are one or more dimensions of 'var'.\n",
      "    \n",
      "    multi_sim(model, trials_list, par_val, random_resp=False, sim_type=None)\n",
      "        Simulate one or more trial sequences from the same schedule with known parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : object\n",
      "            Model to use.\n",
      "        \n",
      "        trials_list : list\n",
      "            List of time step level experimental data (cues, outcomes\n",
      "            etc.) for each participant.  These should be generated from\n",
      "            the same experimental schedule.\n",
      "        \n",
      "        par_val : list\n",
      "            Learning model parameters (floats or ints).\n",
      "            \n",
      "        random_resp : boolean\n",
      "            Should responses be random?\n",
      "        \n",
      "        sim_type: str or None, optional\n",
      "            Type of simulation to perform (passed to the model's .simulate() method).\n",
      "            Should be a string indicating the type of simulation if there is more than\n",
      "            one type (e.g. latent cause models), and otherwise should be None.\n",
      "            Defaults to None.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ds : dataset\n",
      "    \n",
      "    oat_grid(model, experiment, free_par, fixed_values, n_points=10, oat=None, n=20)\n",
      "        Compute ordinal adequacy test (OAT) scores while varying one model parameter\n",
      "        (at evenly spaced intervals across its entire domain) and keeping the other parameters fixed.\n",
      "        Useful for examining model behavior via plots.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model: learning model object\n",
      "        \n",
      "        experiment: experiment\n",
      "        \n",
      "        free_par: str\n",
      "            Name of parameter to vary.\n",
      "            \n",
      "        fixed_values: dict\n",
      "            Dict of values to be given to fixed parameters (keys are\n",
      "            parameter names).\n",
      "            \n",
      "        n_points: int, optional\n",
      "            How many values of the free parameter should be\n",
      "            used.  Defaults to 10.\n",
      "        \n",
      "        oat: str, optional\n",
      "        \n",
      "        n: int, optional\n",
      "            Number of individuals to simulate.  Defaults to 20.\n",
      "            \n",
      "        Returns\n",
      "        -------\n",
      "        df: data frame\n",
      "            Parameter combinations with their mean OAT scores.\n",
      "    \n",
      "    one_step_pred(model, ds, n_pred=10, method='indv')\n",
      "        One step ahead prediction test (similar to cross-validation).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        ds : dataset (xarray)\n",
      "            Dataset of time step level experimental data (cues, outcomes etc.)\n",
      "            for each participant.\n",
      "        n_pred : int\n",
      "            The number of trials to be predicted (at the end of each data\n",
      "            set).\n",
      "        method : string\n",
      "            The method used to fit the model, either \"indv\" or \"em\".  Defaults\n",
      "            to 'indv'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This tests how well each of the last few choices is predicted by the model when fit to preceding trials.\n",
      "        \n",
      "        For now, this assumes discrete choice data (i.e. resp_type = 'choice')\n",
      "        \n",
      "        It is based on the 'prediction method' of Yechiam and Busemeyer (2005).\n",
      "        \n",
      "        We assume that each trial/response sequence has the same length.\n",
      "    \n",
      "    perform_oat(model, experiment, minimize=True, oat=None, n=5, max_time=60, verbose=False, algorithm=6, sim_type=None)\n",
      "        Perform an ordinal adequacy test (OAT).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model: learning model object\n",
      "        \n",
      "        experiment: experiment\n",
      "        \n",
      "        minimize: boolean, optional\n",
      "            Should the OAT score by minimized as well as maximized?\n",
      "            Defaults to True.\n",
      "        \n",
      "        oat: str or None, optional\n",
      "            Name of the OAT to use.  Defaults to None, in which\n",
      "            case the alphabetically first OAT in the experiment.\n",
      "        \n",
      "        n: int, optional\n",
      "            Number of individuals to simulate.  Defaults to 5.\n",
      "                \n",
      "        max_time: int, optional\n",
      "            Maximum time for each optimization (in seconds), i.e.\n",
      "            about half the maximum total time running the whole OAT should take.\n",
      "            Defaults to 60.\n",
      "            \n",
      "        verbose: boolean, optional\n",
      "            Should the parameter values be printed as the search is going on?\n",
      "            Defaults to False.\n",
      "            \n",
      "        algorithm: object, optional\n",
      "            NLopt algorithm to use for optimization.\n",
      "            Defaults to nlopt.GN_ORIG_DIRECT.\n",
      "            \n",
      "        sim_type: str or None, optional\n",
      "            Type of simulation to perform (passed to the model's .simulate() method).\n",
      "            Should be a string indicating the type of simulation if there is more than\n",
      "            one type (e.g. latent cause models), and otherwise should be None.\n",
      "            Defaults to None.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        output: dataframe (Pandas)\n",
      "            Model parameters that produce maximum and minimum mean OAT score,\n",
      "            along with those maximum and minimum mean OAT scores and (if n > 1)\n",
      "            their associated 95% confidence intervals.\n",
      "            \n",
      "        mean_resp_max: dataframe\n",
      "            Relevant responses at OAT maximum (and minimum if applicable), averaged\n",
      "            across individuals and trials.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The experiment's OAT object defines a behavioral score function\n",
      "        designed such that positive values reflect response patterns\n",
      "        consistent with empirical data and negative values reflect the\n",
      "        opposite.  This method maximizes and minimizes the score produced\n",
      "        by the learning model.  If the maximum score is positive, the model\n",
      "        CAN behave reproduce empirical results.  If the minimum score is\n",
      "        also positive, the model ALWAYS reproduces those results.\n",
      "    \n",
      "    recovery_test(model, experiment, schedule=None, a_true=1, b_true=1, n=10, method='indv')\n",
      "        Perform a parameter recovery test.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : object\n",
      "            Learning model.\n",
      "            \n",
      "        experiment : object\n",
      "            The experiment to be used for the recovery test.\n",
      "            \n",
      "        schedule : str, optional\n",
      "            Name of the experimental schedule to be used for the test.\n",
      "            Defaults to the first schedule in the experiment definition.\n",
      "            \n",
      "        a_true : int or list, optional\n",
      "            Hyperarameter of the beta distribution used to generate true\n",
      "            parameters.  Can be either a scalar or a list equal in length\n",
      "            to the the number of parameters.  Defaults to 1.\n",
      "            \n",
      "        b_true : int or list, optional\n",
      "            Hyperarameter of the beta distribution used to generate true\n",
      "            parameters.  Can be either a scalar or a list equal in length\n",
      "            to the the number of parameters.  Defaults to 1.\n",
      "            \n",
      "        n : int, optional\n",
      "            Number of individuals to simulate.  Defaults to 10.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        A parameter recovery test consists of three steps:\n",
      "        1) generate random parameter vectors (simulated individuals)\n",
      "        2) simulate data for each parameter vector\n",
      "        3) fit the model to the simulated data to estimate individual parameters\n",
      "        4) compare the estimated parameters (from step 3) to the true ones (from step 1)\n",
      "        This procedure allows one to test how well a given learning model's parameters\n",
      "        can be identified from data.  Some models and experimental schedules will have\n",
      "        better estimation properties than others.\n",
      "        \n",
      "        For now, this assumes discrete choice data (i.e. resp_type = 'choice').\n",
      "    \n",
      "    split_pred(model, trials_list, eresp_list, t_fit, method='indv')\n",
      "        Split prediction test (similar to cross-validation).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        trials_list : list\n",
      "            List of time step level experimental data (cues, outcomes\n",
      "            etc.) for each participant.\n",
      "        eresp_list : list\n",
      "            List of empirical response arrays for each participant.\n",
      "        t_fit : int\n",
      "            The first 't_fit' trials are used to predict the remaining\n",
      "            ones.\n",
      "        method : string\n",
      "            The method used to fit the model, either \"indv\" or \"em\".\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For now, this assumes discrete choice data (i.e. resp_type = 'choice').\n",
      "        \n",
      "        This is similar to the 'one_step_pred' method described above, but simply predict the last part of the data from the first.\n",
      "        \n",
      "        It is thus much faster to run and (at least for now) more practical.\n",
      "\n",
      "FILE\n",
      "    /Users/sam/Dropbox/Research/Modeling/PhD thesis/statsrat/statsrat/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resp_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-aa3dbef96586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msim_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_sim_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrn_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/Research/Modeling/PhD thesis/statsrat/statsrat/__init__.py\u001b[0m in \u001b[0;36mmake_sim_data\u001b[0;34m(model, experiment, schedule, a_true, b_true, n)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[0mds_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m         ds_list += [model.simulate(trials_list[i],\n\u001b[0m\u001b[1;32m   1059\u001b[0m                                    \u001b[0mpar_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true_par'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m                                    \u001b[0mrandom_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Research/Modeling/PhD thesis/statsrat/statsrat/rw/__init__.py\u001b[0m in \u001b[0;36msimulate\u001b[0;34m(self, trials, par_val, random_resp, ident)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mresp_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'choice'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m                 \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resp_type' is not defined"
     ]
    }
   ],
   "source": [
    "sim_data = sr.make_sim_data(model = drva, experiment = lrn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(CompAct.simulate)\n",
    "print(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = sim_data['ds'].to_dataframe()\n",
    "print(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method simulate in module statsrat.rw:\n",
      "\n",
      "simulate(trials, par_val=None, random_resp=False, ident='sim') method of statsrat.rw.model instance\n",
      "    Simulate a trial sequence once with known model parameters.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    trials: dataset (xarray)\n",
      "        Time step level experimental data (cues, outcomes etc.).\n",
      "    \n",
      "    par_val: list, optional\n",
      "        Learning model parameters (floats or ints).\n",
      "    \n",
      "    random_resp: str, optional\n",
      "        Whether or not simulated responses should be random.  Defaults\n",
      "        to false, in which case behavior (b) is identical to expected\n",
      "        behavior (b_hat); this saves some computation time.  If true\n",
      "        and resp_type is 'choice', then discrete responses are selected\n",
      "        using b_hat as choice probabilities.  If true and resp_type is\n",
      "        'exct' or 'supr' then a small amount of normally distributed\n",
      "        noise (sd = 0.01) is added to b_hat.\n",
      "    \n",
      "    ident: str, optional\n",
      "        Individual participant identifier.  Defaults to 'sim'.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    ds: dataset\n",
      "        Simulation data.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The response type is determined by the 'resp_type' attribute of the 'trials' object.\n",
      "    \n",
      "    The response type 'choice' is used for discrete response options.  This\n",
      "    produces response probabilities using a softmax function:\n",
      "    .. math::       ext{resp}_i = \f",
      "rac{ e^{\\phi \\hat{u}_i} }{ \\sum_j e^{\\phi \\hat{u}_j} }\n",
      "    \n",
      "    The response type 'exct' is used for excitatory Pavlovian\n",
      "    conditioning:\n",
      "    .. math::       ext{resp} = \f",
      "rac{ e^{\\phi \\hat{u}_i} }{ e^{\\phi \\hat{u}_i} + 1 }\n",
      "    \n",
      "    The response type 'supr' (suppression) is used for inhibitory\n",
      "    Pavlovian conditioning:\n",
      "    .. math::       ext{resp} = \f",
      "rac{ e^{-\\phi \\hat{u}_i} }{ e^{-\\phi \\hat{u}_i} + 1 }\n",
      "    \n",
      "    Here :math:`\\phi` represents the 'resp_scale' parameter.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CompAct.simulate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a subset of the data for compare optimization algorithms etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST MODEL (**MODEL NAME**)\n",
    "\n",
    "# Test different optimization algorithms (subset of data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how long the optimization algorithm needs to run (subset of data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data (full dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND MODEL (**MODEL NAME**)\n",
    "\n",
    "# Test different optimization algorithms (subset of data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how long the optimization algorithm needs to run (subset of data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data (full dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare AIC (Akaike Information Criterion) values\n",
    "# These are based on a log-likelihood but penalize the number of free parameters\n",
    "# Higher is better\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
